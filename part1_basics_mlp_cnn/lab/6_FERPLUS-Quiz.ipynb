{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Facial Emotion Recognition - QUIZ\n",
    "\n",
    "Fill in the statements that have `??`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Facial Emotion Recognition - *face is the index of mind*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Uses Deep Neural Nets to identify Facial Emotion.\n",
    "- Classifies into one of 8 Emotions based on Paul Ekman 6 universal emotions.  \n",
    " **| neutral | happiness | surprise | sadness | anger | disgust | fear | contempt |**\n",
    "- Kaggle Competition with human labeled dataset in 2013.\n",
    "- Microsoft researchers crowd sourced tags(labels) - FER+ Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Paul Ekman American psychologist identified 6 emotions that are universal across different cultures and developed a coding mechanism FACS(Facial Action coding scheme)\n",
    "- Microsoft researchers relabeled the dataset using crowding sourcing, they applied different approaches such as majority-voting, multi-labeling, probablistic label drawing and cross-entropy loss.\n",
    "- Without going into the details of the paper, a few things to note "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## FER+\n",
    "- Used Convolutional Neural Nets and VGG13 as the DNN network.\n",
    "- Used Cross Entropy to measure the loss.\n",
    "- FER+ has new corrected labels\n",
    "- Preprocess the dataset by augmenting the images -  {crop faces, scale, shift, normalize, ..}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## References\n",
    "- [FER from scratch using Apache MXNet](https://github.com/TalkAI/facial-emotion-recognition-gluon)\n",
    "- Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution https://arxiv.org/abs/1608.01041"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transfer Learning\n",
    "- Small Datasets - Overfitting\n",
    "- Start with a pre-trained model, instead of from scratch - **knowledge transfer**\n",
    "- Saves cost and time on labeling efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    " import mxnet as mx\n",
    "import numpy as np\n",
    "import os, time, shutil\n",
    "import zipfile, os\n",
    "from gluoncv.utils import download\n",
    "\n",
    "from mxnet import gluon, image, init, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "from gluoncv.utils import makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "data_url = 'https://s3.amazonaws.com/odsc-conf/data.zip'\n",
    "data_zip = download(data_url, path='./')\n",
    "with zipfile.ZipFile(data_zip, 'r') as zipped_data:\n",
    "    zipped_data.extractall(os.path.expanduser('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "processed_train_images = np.load('./data/fer_train_processed_images.npy')\n",
    "processed_train_labels = np.load('./data/fer_train_processed_labels.npy')\n",
    "processed_test_images = np.load('./data/fer_test_processed_images.npy')\n",
    "processed_test_labels = np.load('./data/fer_test_processed_labels.npy')\n",
    "processed_val_images = np.load('./data/fer_val_processed_images.npy')\n",
    "processed_val_labels = np.load('./data/fer_val_processed_labels.npy')\n",
    "\n",
    "train_labels = np.argmax(processed_train_labels, axis=1)\n",
    "val_labels = np.argmax(processed_val_labels, axis=1)\n",
    "test_labels = np.argmax(processed_test_labels, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# FER Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(processed_train_images.shape, processed_train_labels.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "emotions = {0: 'neutral', 1: 'happiness', 2: 'surprise',3: 'sadness',\n",
    "            4: 'anger', 5: 'disgust', 6: 'fear', 7: 'contempt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(processed_train_images[100].reshape(64,64), cmap='gray')\n",
    "print(processed_train_labels[100])\n",
    "print(emotions[np.argmax(processed_train_labels[100])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "CPU_COUNT = cpu_count()\n",
    "\n",
    "per_device_batch_size = 64\n",
    "\n",
    "num_gpu_range = mx.test_utils.list_gpus()\n",
    "\n",
    "ctx = [mx.gpu(i) for i in num_gpu_range] \\\n",
    "if num_gpu_range.stop > num_gpu_range.start else mx.cpu()\n",
    "\n",
    "batch_size = per_device_batch_size* max(num_gpu_range.stop, 1)\n",
    "\n",
    "\n",
    "print('num_cpu: {}, num_gpus: {},\\n ctx: {}, batch_size:{}'.format(CPU_COUNT, num_gpu_range.stop, ctx, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset, DataLoader and Transforms\n",
    "[Dataset](https://mxnet.incubator.apache.org/api/python/gluon/data.html#mxnet.gluon.data.Dataset) - used to represent collection of data, includes methods to load/parse data on disk.  \n",
    "[DataLoader](https://mxnet.incubator.apache.org/api/python/gluon/data.html#mxnet.gluon.data.DataLoader) - mini-batches of data from Dataset, iterator interface, can load data in parallel.  \n",
    "[Transforms](https://mxnet.incubator.apache.org/api/python/gluon/data.html#vision-transforms) - Transformations that can applied on data for augmentation, multiple transforms can composed to apply sequentially on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transforms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 1. Gray scale to 3 Channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gray_to_3channel = lambda x: nd.concat(nd.array(x), \\\n",
    "                        nd.array(x), nd.array(x), dim = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 2. Resize images from (64x64) to (224 x224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "center_pad = lambda x: nd.pad(nd.array(x).expand_dims(axis=0),\\\n",
    "      'constant',pad_width=(0, 0, 0, 0, 80,80, 80,80)). \\\n",
    "      squeeze().expand_dims(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 3. Compose a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([center_pad, gray_to_3channel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_data = gluon.data.DataLoader(gluon.data.ArrayDataset \\\n",
    "(processed_train_images, train_labels).transform_first(transformer),\n",
    "batch_size, shuffle=True, num_workers=CPU_COUNT)\n",
    "\n",
    "val_data = gluon.data.DataLoader(gluon.data.ArrayDataset \\\n",
    "(processed_val_images, val_labels).transform_first(transformer),\n",
    "batch_size, shuffle=True, num_workers=CPU_COUNT)\n",
    "\n",
    "test_data = gluon.data.DataLoader(gluon.data.ArrayDataset \\\n",
    "(processed_test_images, test_labels).transform_first(transformer),\n",
    "batch_size, shuffle=True, num_workers=CPU_COUNT, last_batch='discard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_shape = None\n",
    "label_shape = None\n",
    "for X_batch, y_batch in train_data:\n",
    "    print(\"X_batch has shape {}, and y_batch has shape {}\".format(X_batch.shape, y_batch.shape))\n",
    "    data_shape = X_batch.shape\n",
    "    label_shape = y_batch.shape\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use Pretrained VGG13 model from GluonCV Model Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'vgg13'\n",
    "from gluoncv.model_zoo import get_model\n",
    "net = get_model(model_name, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Change the number of classes in the output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "classes = 8\n",
    "with net.name_scope():\n",
    "    net.output = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Initialize the Parameters of the Output Layer(only) using Xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Re assign parameters to other contexts\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "# Hybridize\n",
    "net.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Verify the shapes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = mx.sym.var('data')\n",
    "sym = net(x)\n",
    "mx.viz.plot_network(sym, \\\n",
    "                    node_attrs={\"shape\":\"oval\",\"fixedsize\":\"false\"}, \\\n",
    "                    shape={\"data\": data_shape})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 0.01\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Create a Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "trainer = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "metric = mx.metric.Accuracy()\n",
    "L = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def test(net, val_data, ctx):\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\n",
    "        outputs = [net(X) for X in data]\n",
    "        metric.update(label, outputs)\n",
    "\n",
    "    return metric.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_batch = len(train_data)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    tic = time.time()\n",
    "    train_loss = 0\n",
    "    metric.reset()\n",
    "\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # use gluon.utils.split_load to load a slice of the mini-batch to each context\n",
    "        data = gluon.utils.split_and_load(??)\n",
    "        label = gluon.utils.split_and_load(??)\n",
    "        with ag.record():\n",
    "            # pass each slice of dadata to the network's forward pass and collect output\n",
    "            outputs = ??\n",
    "            loss = [L(yhat, y) for yhat, y in zip(outputs, label)]\n",
    "        # for each loss, compute gradients    \n",
    "        for l in loss:\n",
    "            l.??\n",
    "        trainer.step(batch_size)\n",
    "        train_loss += sum([l.mean().asscalar() for l in loss]) / len(loss)\n",
    "\n",
    "        metric.update(label, outputs)\n",
    "\n",
    "    _, train_acc = metric.get()\n",
    "    train_loss /= num_batch\n",
    "    \n",
    "    _, val_acc = test(net, val_data, ctx)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print('[Epoch %d] Train-acc: %.3f, loss: %.3f | Val-acc: %.3f | time: %.1f' %\n",
    "             (epoch, train_acc, train_loss, val_acc, time.time() - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training and Validataion Loss/Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = plt.figure(figsize=(12,6))\n",
    "fg1 = f.add_subplot(121)\n",
    "fg2 = f.add_subplot(122)\n",
    "\n",
    "fg1.set_xlabel('epoch',fontsize=14)  \n",
    "fg1.set_title('Loss over Training')\n",
    "fg1.grid(True, which=\"both\")\n",
    "fg1.plot(range(epochs), train_losses)\n",
    "\n",
    "fg2.set_title('Comparing accuracy')\n",
    "fg2.set_xlabel('epoch', fontsize=14)\n",
    "fg2.grid(True, which=\"both\")\n",
    "\n",
    "p1, = fg2.plot(range(epochs), train_accuracies)\n",
    "p2, = fg2.plot(range(epochs), val_accuracies)\n",
    "fg2.legend([p1, p2], ['training accuracy', 'validation accuracy'],fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "_, test_acc = test(net, test_data, ctx)\n",
    "print('[Finished] Test-acc: %.3f' % (test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def readImage(file_name):\n",
    "    \"\"\"\n",
    "    Decodes an Image bytearray into 3-D numpy array.\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import io\n",
    "    from array import array\n",
    "    # read the bytearray using OpenCV and convert to RGB\n",
    "    img = Image.open(file_name)\n",
    "    img = img.convert('RGB')\n",
    "    #resize the image to 224x224\n",
    "    img = img.resize((224, 224), Image.ANTIALIAS)\n",
    "    # reshape the array from (height, width, channel) to (channel, height, width)\n",
    "    img = np.swapaxes(img, 0, 2)\n",
    "    img = np.swapaxes(img, 1, 2)\n",
    "    # add a new axis to hold a batch of images.\n",
    "    img = img[np.newaxis, :]\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "def showImage(file_name):\n",
    "    img = Image.open(file_name)\n",
    "    imshow(np.asarray(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Run Prediction on './data/test.jpg' using the trained model above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showImage('./data/test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use the readImage routine above\n",
    "test_img = readImage('./data/test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# verify Input Image shape\n",
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the input image to one of the contexts\n",
    "test_nd = ??\n",
    "\n",
    "# Predict by running forward pass on the network,\n",
    "# simply pass the input data net\n",
    "test_out = ??\n",
    "\n",
    "print('prediction: {}\\n'.format(test_out))\n",
    "\n",
    "# turn output into probabilities\n",
    "test_out = ??\n",
    "\n",
    "print('probabilities: {}\\n'.format(test_out))\n",
    "\n",
    "\n",
    "# top-1 output\n",
    "a = np.argmax(test_out)\n",
    "\n",
    "print('\\nFER Model Output = {}\\n'.format(emotions[a]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
