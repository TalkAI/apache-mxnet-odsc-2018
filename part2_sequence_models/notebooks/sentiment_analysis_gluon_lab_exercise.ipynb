{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Apache MXNet GLUON - Lab Exercise\n",
    "\n",
    "Fill in the statements that have `??`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisites\n",
    "\n",
    "1. Apache MXNet >= v1.3\n",
    "2. Gluon-NLP >=v0.3\n",
    "3. Spacy - Natural Language Processing Utility\n",
    "\n",
    "Apache MXNet [1] and Gluon-NLP [2] are already pre-installed. In the next section, install Spacy [3] and setup resources for English Language Model.\n",
    "\n",
    "In this tutorial, we will use Spacy for Sentence Tokenizer and Language Model.\n",
    "\n",
    "**Credits:** This notebook is borrowed from GLUON-NLP Tutorials [4] and enhanced for this workshop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Install Spacy\n",
    "pip install spacy -U --quiet\n",
    "\n",
    "# Download Spacy resources for English Language Model\n",
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "Given an input text, classify its sentiment as positive of negative.\n",
    "X -> Input Text\n",
    "Y -> Probability. <=0.5: Negative, >0.5: Positive\n",
    "\n",
    "# Solution\n",
    "1. Use IMDB movie review dataset [5] for training the model. IMDB movie review dataset is a curated collection of 25,000 movie reviews (positive and negative) for training and 25,000 for testing. \n",
    "2. Use pre-trained Vocabulary, Embedding and Language Model trained on wikitext2 [2]. This pretrained model is based on LSTM with 200 hidden units. Essentially, we will be using a pre-trained LSTM model for English Vocabulary, Embeddings and Language Model based on wikitext2. \n",
    "3. Use Spacy and GluonNLP for data preparation.\n",
    "4. Use Gluon for defining a simple Neural Network - Embedding -> Encoding (LSTM) -> Dropout -> Dense -> Softmax\n",
    "5. Train and test the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "\n",
    "import gluonnlp as nlp\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MXNet Context. Use mx.cpu() for CPU. Use mx.gpu(0) for 1 GPU\n",
    "context = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Pretrained wikitext-2 Language Model\n",
    "\n",
    "We use a pretrained model on wikitext-2 dataset [6]. Specifically, we use Vocabulary, Language Model i.e., Embeddings and Encodings (LSTM weights) based on wikitext-2 dataset.\n",
    "\n",
    "**Intuition:** Using pretrained language model weights is a common approach for semi-supervised learning in NLP. In order to do a good job with large language modeling on a large corpus of text, our model must learn representations that contain information about the structure of natural language. Intuitively, by starting with these good features, vs random features, weâ€™re able to converge faster upon a good model for our downsteam task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_name = 'standard_lstm_lm_200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 'pretrained', 'standard_lstm_lm_200' language model for 'wikitext-2' dataset.\n",
    "\n",
    "lm_model, vocab = nlp.model.get_model(name=??,\n",
    "                                      dataset_name=??,\n",
    "                                      pretrained=??,\n",
    "                                      ctx=context,\n",
    "                                      dropout=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data pipeline\n",
    "\n",
    "* Load IMDB reviews dataset\n",
    "* Label negative reviews (score <= 5) as o\n",
    "* Label positive reviews (score > 5) as 1\n",
    "* Tokenize using spaCy- Extract words, punctuation marks from review text\n",
    "* Convert each token to an index in the vocabulary. Vocabulary is obtained from wikitext2\n",
    "* Prepare data iterators that can iterate on training and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the train and test IMDB movie review dataset\n",
    "train_dataset, test_dataset = [nlp.data.IMDB(root='data/imdb', segment=segment)\n",
    "                               for segment in ('train', 'test')]\n",
    "\n",
    "# Use spaCy English (en) tokenizer on input sentences to get tokens(words and punctuation marks)\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "\n",
    "# Clip sentences to be max 500 tokens. Use Gluon-NLP ClipSequence\n",
    "length_clip = ??\n",
    "\n",
    "def preprocess(x):\n",
    "    \"\"\"\n",
    "    1. Prepare labels. label = 1 (positive) if score > 5. label = 0 (negative) if score <= 5.\n",
    "    2. Tokenize - Extract words, punctuation marks from review text.\n",
    "    3. Convert each token to an index in the vocabulary.\n",
    "    \"\"\"\n",
    "    data, label = x\n",
    "    label = int(label > 5)\n",
    "    # Tokenize the data\n",
    "    tokenized_data = ??\n",
    "    # Clip the tokens\n",
    "    tokenized_clipped_data = ??\n",
    "    # Get vocabulary indexes for the tokens. Use pre-loaded 'vocab'.\n",
    "    data = ??\n",
    "    return data, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    \n",
    "    return dataset, lengths\n",
    "\n",
    "# Preprocess the dataset\n",
    "print(\"Preparing Train dataset. This will take few minutes...\")\n",
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "\n",
    "print(\"Preparing Test dataset. This will take few minutes...\")\n",
    "test_dataset, test_data_lengths = preprocess_dataset(test_dataset)\n",
    "\n",
    "print(\"Data is ready!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Prepare Dataloader\n",
    "\n",
    "* Input sentences can be of different lengths.\n",
    "* Use FixedBucketSampler, which assigns each data sample to a fixed bucket based on its length.\n",
    "* Batchify function (batchify) is applied on all the samples as the loaders read the batches.\n",
    "* We apply *Pad* for padding smaller length sequence to max length sequence in the bucket.\n",
    "* We apply *Stack* for stacking data, label, data_length i.e., [sentence, sentiment label, sentence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "bucket_num, bucket_ratio = 10, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader():\n",
    "    batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(axis=0, ret_length=True),\n",
    "        nlp.data.batchify.Stack(dtype='float32'))\n",
    "    \n",
    "    # We want to use a FixedBucketSampler for sampling the Training Data.\n",
    "    # Hint: Use nlp.data.sampler.FixedBucketSampler, train_data_lengths, batch_size, bucket_num, bucket_ratio.\n",
    "    # Also, make sure to shuffle!\n",
    "    batch_sampler = ??\n",
    "    \n",
    "    print(batch_sampler.stats())\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn, num_workers=4)\n",
    "    test_dataloader = gluon.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn, num_workers=4)\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Network\n",
    "\n",
    "* **Embedding, LSTM Layer:** To use pre-trained weights, we base our network on the Language Model Network (Embedding -> LSTM). \n",
    "* **Mean Pooling Layer:** We have multiple words input (reviews) and one output (sentiment). Hence, we average(mean) states across all time steps into one value.\n",
    "* **Dense Layer:** To generate the final output\n",
    "\n",
    "![Network Structure](network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolingLayer(gluon.HybridBlock):\n",
    "    \"\"\"A block for mean pooling of encoder features\"\"\"\n",
    "    def __init__(self, prefix=None, params=None):\n",
    "        super(MeanPoolingLayer, self).__init__(prefix=prefix, params=params)\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length):\n",
    "        masked_encoded = F.SequenceMask(data,\n",
    "                                        sequence_length=valid_length,\n",
    "                                        use_sequence_length=True)\n",
    "        agg_state = F.broadcast_div(F.sum(masked_encoded, axis=0),\n",
    "                                    F.expand_dims(valid_length, axis=1))\n",
    "        return agg_state\n",
    "\n",
    "\n",
    "class SentimentNet(gluon.HybridBlock):\n",
    "    \"\"\"Network for sentiment analysis.\"\"\"\n",
    "    def __init__(self, prefix=None, params=None):\n",
    "        super(SentimentNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            self.embedding = None # will set with lm embedding later\n",
    "            self.encoder = None # will set with lm encoder later\n",
    "            self.agg_layer = MeanPoolingLayer()\n",
    "            self.output = gluon.nn.HybridSequential()\n",
    "            with self.output.name_scope():\n",
    "                ## Add a Dense Layer.\n",
    "                ## Hint1: Consider using gluon.nn module.\n",
    "                ## Hint2: We expect last dense layer to output 1 probability output. Let Dense layer have 1 unit and we will not flatten it. \n",
    "                self.output.add(??)\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length): \n",
    "        ## Pass the data through embedding layer\n",
    "        embedded = ??\n",
    "        ## Pass the embedded layer output through encoding layer\n",
    "        encoded = ??\n",
    "        ## Aggregate the encoded layer output.\n",
    "        ## Hint: You need 'valid_length' for aggregating the output.\n",
    "        agg_state = ??\n",
    "        ## Finally get the output from 'output' dense layer.\n",
    "        out = ??\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Initialize Network with Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SentimentNet()\n",
    "\n",
    "# Use Pretrained Embeddings from wikitext-2 Language model\n",
    "net.embedding = lm_model.??\n",
    "\n",
    "# Use Pretrained Encoder states (LSTM) from wikitext-2\n",
    "net.encoder = lm_model.??\n",
    "\n",
    "net.hybridize()\n",
    "\n",
    "# Random initialize the last Dense Layer using Xavier initializer\n",
    "net.output.initialize(mx.init.Xavier(), ctx=context)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "epochs = 3\n",
    "grad_clip = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, dataloader, context):\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "    total_L = 0.0\n",
    "    total_sample_num = 0\n",
    "    total_correct_num = 0\n",
    "    print('Begin Testing...')\n",
    "    for i, ((data, valid_length), label) in enumerate(dataloader):\n",
    "        # Step 1: Prepare data\n",
    "        data = mx.nd.transpose(data.as_in_context(context))\n",
    "        valid_length = valid_length.as_in_context(context).astype(np.float32)\n",
    "        label = label.as_in_context(context)\n",
    "        \n",
    "        # Step 2: Forward pass\n",
    "        output = net(data, valid_length)\n",
    "        \n",
    "        # Step 3: Calculate loss\n",
    "        L = loss(output, label)\n",
    "        \n",
    "        # Step 4: Statistics - Keeping moving average loss and accuracy\n",
    "        pred = (output > 0.5).reshape(-1)\n",
    "        total_L += L.sum().asscalar()\n",
    "        total_sample_num += label.shape[0]\n",
    "        total_correct_num += (pred == label).sum().asscalar()\n",
    "    avg_L = total_L / float(total_sample_num)\n",
    "    acc = total_correct_num / float(total_sample_num)\n",
    "    return avg_L, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, context, epochs):\n",
    "    # Use Follow the Moving Leader Optimizer - [7]\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'ftml',\n",
    "                            {'learning_rate': learning_rate})\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "    parameters = net.collect_params().values()\n",
    "    print(\"Training the Sentiment Classification Model...\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_L = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        print(\"[Epoch - {}]\".format(epoch))\n",
    "        for i, ((data, length), label) in enumerate(train_dataloader):\n",
    "            L = 0\n",
    "            with autograd.record():\n",
    "                # Step 1: Forward pass\n",
    "                output = net(data.as_in_context(context).T,\n",
    "                             length.as_in_context(context)\n",
    "                                   .astype(np.float32))\n",
    "                # Step 2: Calculate Loss\n",
    "                L = L + loss(output, label.as_in_context(context)).mean()\n",
    "            \n",
    "            # Step 3: Backward pass\n",
    "            L.backward()\n",
    "            \n",
    "            # Step 3.1: Clip gradient - Avoid gradient explosion\n",
    "            if grad_clip:\n",
    "                gluon.utils.clip_global_norm(\n",
    "                    [p.grad(context) for p in parameters],\n",
    "                    grad_clip)\n",
    "            \n",
    "            # Step 4: Do parameter updates\n",
    "            trainer.step(1)\n",
    "            \n",
    "            # For epoch statistics - Loss and data sample count\n",
    "            epoch_sent_num += data.shape[1]\n",
    "            epoch_L += L.asscalar()\n",
    "    \n",
    "        print('Train Avg Loss {:.6f}'.format(epoch_L / epoch_sent_num))\n",
    "        \n",
    "        # Step 5: Evaluation after each epoch\n",
    "        test_avg_L, test_acc = evaluate(net, test_dataloader, context)\n",
    "        print('Test Acc {:.2f}, Test Avg Loss {:.6f}'.format(test_acc, test_avg_L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train(net, context, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Positive Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob1 = net(\n",
    "            mx.nd.reshape(\n",
    "            mx.nd.array(vocab[['This', 'movie', 'is', 'good']], ctx=context),\n",
    "            shape=(-1, 1)), mx.nd.array([4], ctx=context)).sigmoid()\n",
    "print(\"Sentiment - \", 'positive' if prob1[0] > 0.5 else 'negative')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Negative Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob2 = net(\n",
    "            mx.nd.reshape(\n",
    "            mx.nd.array(vocab[['Movie', 'was', 'bad', 'and', 'boring']], ctx=context),\n",
    "            shape=(-1, 1)), mx.nd.array([5], ctx=context)).sigmoid()\n",
    "print(\"Sentiment - \", 'positive' if prob2[0] > 0.5 else 'negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. http://mxnet.incubator.apache.org/\n",
    "2. https://gluon-nlp.mxnet.io\n",
    "3. https://spacy.io/usage/\n",
    "4. https://gluon-nlp.mxnet.io/examples/sentiment_analysis/sentiment_analysis.html\n",
    "5. http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "6. https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset\n",
    "7. https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.FTML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
