{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MXNet Model Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This Notebook is borrowed from: https://github.com/TalkAI/facial-emotion-recognition-gluon/blob/master/notebooks/Gluon_FERPlus.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Install the required Packages locally\n",
    "\n",
    "We will need the PyPi packages listed below to test model server locally, and to perform image pre-processing prior to the model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q mxnet-model-server\n",
    "print('mxnet-model-server installed')\n",
    "!pip install -q scikit-image==0.13.0\n",
    "print('scikit-learn installed')\n",
    "!pip install -q opencv-python\n",
    "print('opencv-python installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"input_type\": \"image/jpeg\", \r\n",
      "  \"inputs\": [\r\n",
      "    {\r\n",
      "      \"data_shape\": [1, 1, 64, 64], \r\n",
      "      \"data_name\": \"data\"\r\n",
      "    }\r\n",
      "  ], \r\n",
      "  \"outputs\": [\r\n",
      "    {\r\n",
      "      \"data_shape\": [1, 8],\r\n",
      "      \"data_name\": \"hybridsequential0_dense2_fwd\"\r\n",
      "    }\r\n",
      "  ], \r\n",
      "  \"output_type\": \"application/json\"\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "# We define the model's input and output type and shape via signature.json\n",
    "!cat ./model_archive_resources/signature.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\r\n",
      "happiness\r\n",
      "surprise\r\n",
      "sadness\r\n",
      "anger\r\n",
      "disgust\r\n",
      "fear\r\n",
      "contempt"
     ]
    }
   ],
   "source": [
    "# We define the model's class label names via synset.txt\n",
    "!cat ./model_archive_resources/synset.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\r\n",
      "from mms.utils.mxnet import image\r\n",
      "from mms.model_service.mxnet_model_service import MXNetBaseService\r\n",
      "from skimage import transform\r\n",
      "import mxnet as mx\r\n",
      "import cv2 as cv\r\n",
      "\r\n",
      "# One time initialization of Haar Cascade Classifier to extract and crop out face\r\n",
      "face_detector = cv.CascadeClassifier('haarcascade_frontalface.xml')\r\n",
      "# Classifier parameter specifying how much the image size is reduced at each image scale\r\n",
      "scale_factor = 1.3\r\n",
      "# Classifier parameter how many neighbors each candidate rectangle should have to retain it\r\n",
      "min_neighbors = 5\r\n",
      "\r\n",
      "def crop_face(image):\r\n",
      "    \"\"\"Attempts to identify a face in the input image.\r\n",
      "\r\n",
      "    Parameters\r\n",
      "    ----------\r\n",
      "    image : array representing a BGR image\r\n",
      "\r\n",
      "    Returns\r\n",
      "    -------\r\n",
      "    array\r\n",
      "        The cropped face, transformed to grayscale. If no face found returns None\r\n",
      "\r\n",
      "    \"\"\"\r\n",
      "    gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\r\n",
      "    face_roi_list = face_detector.detectMultiScale(gray_image, scale_factor, min_neighbors)\r\n",
      "    \r\n",
      "    if (len(face_roi_list) > 0):\r\n",
      "        (x,y,w,h) = face_roi_list[0]\r\n",
      "        return gray_image[y:y+h,x:x+w]\r\n",
      "    else:\r\n",
      "        return None\r\n",
      "\r\n",
      "class FERService(MXNetBaseService):\r\n",
      "    \"\"\"\r\n",
      "    Defines custom pre and post processing for the Facial Emotion Recognition model\r\n",
      "    \"\"\"\r\n",
      "\r\n",
      "    def _preprocess(self, data):\r\n",
      "        \"\"\"\r\n",
      "        Pre-process requests by attempting to extract face image, and transforing to fit the model's input\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        data : list of input images\r\n",
      "            Raw inputs from request.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        list of NDArray\r\n",
      "            Processed images in the model's expected input shape\r\n",
      "        \"\"\"\r\n",
      "        img_list = []\r\n",
      "\r\n",
      "        # Iterate over all input images provided with the request, transform and append for inference\r\n",
      "        for idx, img in enumerate(data):          \r\n",
      "            input_shape = self.signature['inputs'][idx]['data_shape']\r\n",
      "            [h, w] = input_shape[2:]\r\n",
      "            img_arr = image.read(img).asnumpy()\r\n",
      "            \r\n",
      "            # Try to identify face to crop\r\n",
      "            face = crop_face(img_arr)\r\n",
      "            if (face is not None):\r\n",
      "                face = transform.resize(face, (h,w))            \r\n",
      "            # If no face identified - use the entire input image\r\n",
      "            else:\r\n",
      "                face = cv.cvtColor(img_arr, cv.COLOR_BGR2GRAY)\r\n",
      "\r\n",
      "            # Transform image into tensor of the required shape \r\n",
      "            face = np.resize(face, input_shape)\r\n",
      "            face = mx.nd.array(face)\r\n",
      "            img_list.append(face)\r\n",
      "        \r\n",
      "        return img_list\r\n",
      "\r\n",
      "    def _postprocess(self, data):\r\n",
      "        \"\"\"\r\n",
      "        Post-process inference result to normalize probabilities and render with labels\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        data : list of NDArray\r\n",
      "            Inference output.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        list of object\r\n",
      "            list of outputs to be sent back.\r\n",
      "        \"\"\"\r\n",
      "        response = []\r\n",
      "        \r\n",
      "        # Iterating over inference results to render the normalized probabilities\r\n",
      "        for inference_result in data:\r\n",
      "            softmax_result = inference_result.softmax().asnumpy()\r\n",
      "            for idx, label in enumerate(self.labels):\r\n",
      "                response.append({label: float(softmax_result[0][idx])})\r\n",
      "        return response\r\n"
     ]
    }
   ],
   "source": [
    "# And lastly, we define custom code for request handling via python code other auxiliary files\n",
    "!cat ./model_archive_resources/fer_service.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%cd model_archive_resources\n",
    "!curl -O https://s3.amazonaws.com/mxnet-demo-models/models/fer/gluon_ferplus-0000.params\n",
    "!curl -O https://s3.amazonaws.com/mxnet-demo-models/models/fer/gluon_ferplus-symbol.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wamy/nswamy/deepengine/workspace/dl-introduction-apache-mxnet-odsc-2018\n",
      "/Users/wamy/nswamy/deepengine/workspace/dl-introduction-apache-mxnet-odsc-2018\r\n"
     ]
    }
   ],
   "source": [
    "%cd ../\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's package everything up into a Model Archive bundle\n",
    "!mxnet-model-export --model-name ferplus \\\n",
    "--service-file-path ./model_archive_resources/fer_service.py \\\n",
    "--model-path ./model_archive_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wamy/nswamy/deepengine/workspace/dl-introduction-apache-mxnet-odsc-2018/part3_model_server/model_archive_resources\n"
     ]
    }
   ],
   "source": [
    "%cd part3_model_server/model_archive_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 wamy  1896053708  35984920 Oct 30 00:03 ferplus.model\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ferplus.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Spawning a new process to run the server\n",
    "import subprocess as sp\n",
    "server = sp.Popen(\"mxnet-model-server --models ferplus=ferplus.model\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"health\":\"healthy!\"}\r\n"
     ]
    }
   ],
   "source": [
    "# Check out the health endpoint\n",
    "!curl http://127.0.0.1:8080/ping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fer_service.py              mms_app.log\r\n",
      "\u001b[34mferplus\u001b[m\u001b[m                     signature.json\r\n",
      "ferplus.model               synset.txt\r\n",
      "haarcascade_frontalface.xml\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"prediction\":[{\"neutral\":0.3882121741771698},{\"happiness\":0.5335745811462402},{\"surprise\":0.0024918382987380028},{\"sadness\":0.05849442631006241},{\"anger\":0.008850693702697754},{\"disgust\":0.0009525333880446851},{\"fear\":0.0007643798599019647},{\"contempt\":0.006659360136836767}]}\r\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://127.0.0.1:8080/ferplus/predict -F \"data=@../happy.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Lastly, we'll terminate the server\n",
    "server.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
